#!/usr/bin/env python
# encoding=utf-8
# Compatibility imports
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import time

import tensorflow as tf
import numpy as np

import common
from common import unzip, read_data_for_lstm_ctc

from utils import sparse_tuple_from as sparse_tuple_from

# Some configs
# Accounting the 0th indice +  space + blank label = 28 characters
num_classes = ord('9') - ord('0') + 1 + 1 + 1
print("num_classes", num_classes)
# Hyper-parameters
num_epochs = 500
num_hidden = 128
num_layers = 1

"""
train_inputs, train_codes = unzip(list(read_data_for_lstm_ctc("train/*.png")))
train_inputs = train_inputs.swapaxes(1, 2)
print('train_inputs.shape', train_inputs.shape)
print("train_codes", train_codes)
targets = [np.asarray(i) for i in train_codes]
# print("targets", targets)
print("train_inputs.shape[1]", train_inputs.shape[1])
# Creating sparse representation to feed the placeholder
print("tttt", targets)
train_targets = sparse_tuple_from(targets)
print(train_targets)
train_seq_len = np.ones(train_inputs.shape[0]) * common.OUTPUT_SHAPE[1]
print(train_seq_len.shape)
# We don't have a validation dataset :(
val_inputs, val_targets, val_seq_len = train_inputs, train_targets, train_seq_len
"""


# THE MAIN CODE!
# load the training or test dataset from disk
def get_data_set(dirname):
    inputs, codes = unzip(list(read_data_for_lstm_ctc(dirname + "/*.png")))
    inputs = inputs.swapaxes(1, 2)
    # print('train_inputs.shape', train_inputs.shape)
    # print("train_codes", train_codes)
    targets = [np.asarray(i) for i in codes]
    # print("targets", targets)
    # print("train_inputs.shape[1]", train_inputs.shape[1])
    # Creating sparse representation to feed the placeholder
    # print("tttt", targets)
    sparse_targets = sparse_tuple_from(targets)
    # print(train_targets)
    seq_len = np.ones(inputs.shape[0]) * common.OUTPUT_SHAPE[1]
    # print(train_seq_len.shape)
    # We don't have a validation dataset :(
    return inputs, sparse_targets, seq_len


train_inputs, train_targets, train_seq_len = get_data_set('train')
test_inputs, test_targets, test_seq_len = get_data_set('test')

graph = tf.Graph()
with graph.as_default():
    global_step = tf.Variable(0, trainable=False)
    learning_rate = tf.train.exponential_decay(common.INITIAL_LEARNING_RATE,
                                               global_step,
                                               common.DECAY_STEPS,
                                               common.LEARNING_RATE_DECAY_FACTOR,
                                               staircase=True)

    # e.g: log filter bank or MFCC features
    # Has size [batch_size, max_stepsize, num_features], but the
    # batch_size and max_stepsize can vary along each step
    inputs = tf.placeholder(tf.float32, [None, None, common.OUTPUT_SHAPE[0]])

    # Here we use sparse_placeholder that will generate a
    # SparseTensor required by ctc_loss op.
    targets = tf.sparse_placeholder(tf.int32)

    # 1d array of size [batch_size]
    seq_len = tf.placeholder(tf.int32, [None])

    # Defining the cell
    # Can be:
    #   tf.nn.rnn_cell.RNNCell
    #   tf.nn.rnn_cell.GRUCell
    cell = tf.nn.rnn_cell.LSTMCell(num_hidden, state_is_tuple=True)

    # Stacking rnn cells
    stack = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers,
                                        state_is_tuple=True)

    # The second output is the last state and we will no use that
    outputs, _ = tf.nn.dynamic_rnn(cell, inputs, seq_len, dtype=tf.float32)

    shape = tf.shape(inputs)
    batch_s, max_timesteps = shape[0], shape[1]

    # Reshaping to apply the same weights over the timesteps
    outputs = tf.reshape(outputs, [-1, num_hidden])

    # Truncated normal with mean 0 and stdev=0.1
    # Tip: Try another initialization
    # see https://www.tensorflow.org/versions/r0.9/api_docs/python/contrib.layers.html#initializers
    W = tf.Variable(tf.truncated_normal([num_hidden,
                                         num_classes],
                                        stddev=0.1))
    # Zero initialization
    # Tip: Is tf.zeros_initializer the same?
    b = tf.Variable(tf.constant(0., shape=[num_classes]))

    # Doing the affine projection
    logits = tf.matmul(outputs, W) + b

    # Reshaping back to the original shape
    logits = tf.reshape(logits, [batch_s, -1, num_classes])

    # Time major
    logits = tf.transpose(logits, (1, 0, 2))

    loss = tf.contrib.ctc.ctc_loss(logits, targets, seq_len)
    cost = tf.reduce_mean(loss)

    optimizer = tf.train.MomentumOptimizer(learning_rate,
                                           momentum=common.MOMENTUM).minimize(cost, global_step=global_step)

    # Option 2: tf.contrib.ctc.ctc_beam_search_decoder
    # (it's slower but you'll get better results)
    decoded, log_prob = tf.contrib.ctc.ctc_greedy_decoder(logits, seq_len)

    # Accuracy: label error rate
    acc = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),
                                          targets))

with tf.Session(graph=graph) as session:
    # Initializate the weights and biases
    tf.initialize_all_variables().run()


    def do_report():
        test_feed = {inputs: test_inputs,
                     targets: test_targets,
                     seq_len: test_seq_len}
        dd, log_probs, accuracy = session.run([decoded[0], log_prob, acc], test_feed)
        # Decoding
        # dd = session.run(decoded, feed_dict=feed)
        str_decoded = ''.join([chr(x) for x in np.asarray(dd[1]) + common.FIRST_INDEX])
        # Replacing blank label to none
        str_decoded = str_decoded.replace(chr(ord('9') + 1), '')
        # Replacing space label to space
        str_decoded = str_decoded.replace(chr(ord('0') - 1), ' ')
        print(str_decoded, accuracy)


    for curr_epoch in xrange(num_epochs):
        train_cost = train_ler = 0
        start = time.time()

        for batch in xrange(common.BATCHES):
            feed = {inputs: train_inputs,
                    targets: train_targets,
                    seq_len: train_seq_len}

            batch_cost, steps, _ = session.run([cost, global_step, optimizer], feed)
            train_cost += batch_cost * common.BATCH_SIZE
            if steps % common.REPORT_STEPS == 0:
                do_report()
                # train_ler += session.run(acc, feed_dict=feed) * common.BATCH_SIZE

        train_cost /= common.TRAIN_SIZE
        train_ler /= common.TRAIN_SIZE

        val_feed = {inputs: train_inputs,
                    targets: train_targets,
                    seq_len: train_seq_len}

        val_cost, val_ler, lr, steps = session.run([cost, acc, learning_rate, global_step], feed_dict=val_feed)

        log = "Epoch {}/{}, steps = {}, train_cost = {:.3f}, train_ler = {:.3f}, val_cost = {:.3f}, val_ler = {:.3f}, time = {:.3f}s, learning_rate = {}"
        print(
            log.format(curr_epoch + 1, num_epochs, steps, train_cost, train_ler, val_cost, val_ler, time.time() - start,
                       lr))
    # Decoding
    d = session.run(decoded[0], feed_dict=feed)
    str_decoded = ''.join([chr(x) for x in np.asarray(d[1]) + common.FIRST_INDEX])
    # Replacing blank label to none
    str_decoded = str_decoded.replace(chr(ord('9') + 1), '')
    # Replacing space label to space
    str_decoded = str_decoded.replace(chr(ord('0') - 1), ' ')

    # print('Original:\n%s' % original)
    print('Decoded:\n%s' % str_decoded)
